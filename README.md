# ğŸ Fruit Freshness Classification â€” VGG16 Transfer Learning Project

This project builds an image classification model that detects **fresh vs. rotten fruits** across 6 classes:

- fresh apples  
- fresh bananas  
- fresh oranges  
- rotten apples  
- rotten bananas  
- rotten oranges  

The goal was to train and fine-tune a model using **PyTorch**, **transfer learning**, **data augmentation**, and **GPU acceleration** to reach at least **92% accuracy**.  
My final model reached **91.79%**, slightly below the assessment threshold but demonstrating strong performance and solid understanding of deep learning workflows.

---

## ğŸ“Œ **Skills Learned**

### **1. Building Custom Datasets in PyTorch**
- Learned to create a custom `Dataset` class  
- Loaded images using `torchvision.io.read_image`  
- Applied preprocessing and label assignment  
- Wrapped data with `DataLoader` for mini-batch training  

### **2. Using Pretrained Models (Transfer Learning)**
- Loaded `VGG16` with `ImageNet` weights  
- Froze base layers to prevent early overfitting  
- Extracted and repurposed parts of the VGG classifier  
- Learned proper architecture modification with `nn.Sequential`

### **3. Fine-Tuning a Pretrained Network**
- Unfroze VGG16â€™s convolutional blocks after initial training  
- Reduced learning rate (`1e-4`) for stable fine-tuning  
- Improved validation accuracy through additional updates  

### **4. GPU-based Training and `torch.compile`**
- Detected and used CUDA when available  
- Accelerated training with `torch.compile()`  
- Learned device-aware coding patterns (`.to(device)`)

### **5. Data Augmentation & Image Preprocessing**
Used a variety of augmentations to prevent overfitting:
- Random rotations  
- Random resized crops  
- Horizontal & vertical flips  
- Color jitter (brightness/contrast/saturation/hue)  
- Normalization with ImageNet stats  
- Random erasing  

This improved model robustness and real-world generalization.

### **6. Model Training Pipeline**
Implemented training loops using helper functions:
- Forward pass  
- Backpropagation  
- Loss computation using `CrossEntropyLoss`  
- Accuracy tracking  
- Validation loop separate from training loop  

### **7. Running and Interpreting Model Assessments**
- Evaluated model performance on a separate dataset  
- Interpreted loss and accuracy metrics  
- Understood assessment thresholds and failure cases  

---

## ğŸ§  **Model Architecture Overview**

VGG16 (pretrained, initially frozen)
â”‚
â”œâ”€â”€ Convolutional Feature Extractor (unchanged)
â”œâ”€â”€ AdaptiveAvgPool2d
â”œâ”€â”€ Flatten
â”œâ”€â”€ First half of VGG classifier (4096 â†’ 4096)
â”œâ”€â”€ Custom classifier:
â”‚ â”œâ”€â”€ Linear (4096 â†’ 500)
â”‚ â”œâ”€â”€ ReLU
â”‚ â””â”€â”€ Linear (500 â†’ 6 classes)


The modelâ€™s final layer outputs **6 class logits** for multiclass classification using **CrossEntropyLoss**.

---

## ğŸ“‚ **Dataset Structure**

data/fruits/
â”œâ”€â”€ train/
â”‚ â”œâ”€â”€ freshapples/
â”‚ â”œâ”€â”€ freshbanana/
â”‚ â”œâ”€â”€ freshoranges/
â”‚ â”œâ”€â”€ rottenapples/
â”‚ â”œâ”€â”€ rottenbanana/
â”‚ â””â”€â”€ rottenoranges/
â””â”€â”€ valid/
â””â”€â”€ (same folders)


---

## ğŸš€ **Training Results**

| Stage | Description | Accuracy |
|-------|-------------|----------|
| Initial Transfer Learning | Base model frozen | ~0.88 |
| After Adding Custom Layers | Stable improvements | ~0.91 |
| Fine-Tuning (Unfreeze VGG) | LR=0.0001 | **0.9179** |

Final score from assessment:

Accuracy required: 0.92
Your accuracy: 0.9179
Result: Just below passing threshold


---

## ğŸ“˜ **Lessons Learned**

- Even small LR changes matter when fine-tuning pretrained networks  
- Data augmentation significantly reduces overfitting  
- Freezing/unfreezing must be timed correctly  
- Custom layers should be small to avoid overfitting on limited data  
- Validation accuracy can fluctuate â€” patience and tuning are essential  

---

## ğŸ”§ **Future Improvements**

- Add more data or stronger augmentations  
- Use a more modern backbone (ResNet50, EfficientNet, ConvNeXt)  
- Use LR schedulers (CosineDecay / ReduceLROnPlateau)  
- Train longer epochs after unfreezing  
- Apply mixup or cutmix for further robustness  

---

## ğŸ **Final Notes**

This assessment project demonstrates practical skills in:
- Deep learning model construction  
- Transfer learning best practices  
- Fine-tuning pretrained architectures  
- Building full training pipelines from scratch  
- Evaluating and interpreting model performance  

Although the assessment accuracy was slightly below 92%, the project successfully shows competent understanding of modern computer vision workflows.

